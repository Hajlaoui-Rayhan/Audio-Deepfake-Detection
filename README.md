# Audio-Deepfake-Detection
In this project, we propose a model for audio deepfake detection that leverages the WaveLM architecture as a feature extractor, followed by two different classification approaches. WaveLM, a powerful self-supervised model, is used to extract rich and diverse audio features from the raw speech signal. The two approaches we explore for classification are Multiple Kernel Learning (MKL) and a Multi-Fusion Attentive (MFA) classifier. Each approach utilizes the features extracted by WaveLM in distinct ways to achieve robust audio deepfake detection.
WavLM is a self-supervised speech model built upon the Wav2vec2 architecture. It comprises a convolutional feature encoder and Transformer encoders. The convolutional feature encoder processes the raw audio waveform and transforms it into a feature sequence. The Transformer encoders consist of multiple layers, with the first layer receiving the features generated by the CNN encoder, and each subsequent layer taking the output of the previous layer as input.
WavLM employs a masked speech denoising and prediction strategy. Noise or overlapping is introduced into the input audio before applying masking. The model is then trained to predict pseudo-labels for the masked frames. This masked speech denoising technique enables WavLM to learn features beyond automatic speech recognition (ASR), such as speaker characteristics and acoustic environments. These features are particularly valuable for audio detection tasks, as fake speech often contains artifacts related to the speaker.
Attentive Statistics Pooling : 
Attentive Statistics Pooling (ASP) was initially introduced for extracting speaker embeddings, demonstrating exceptional performance in speaker verification tasks. ASP is a pooling technique that merges the benefits of attention mechanisms with statistical pooling. By incorporating higher-order statistics, ASP enhances speaker discriminability, which is particularly useful for identifying speaker-related artifacts in spoofed audio samples.
The Multi-Fusion Attentive (MFA) classifier, which leverages the Attentive Statistics Pooling (ASP) layer. The MFA classifier is composed of stacked time-wise ASP (T-ASP) layers and a single layer-wise ASP (L-ASP) layer. The T-ASP layers extract time-level features from the hidden representations of various Transformer layers, with each T-ASP layer computing a concatenated vector representation of the mean and standard deviation of the input sequence. The L-ASP layer then integrates all the layer-level features to generate the final output representation.
Multiple Kernel Learning Classifier :
The MKL classifier leverages the multiple outputs from different layers of the WavLM model and applies a kernel-based approach for feature fusion and classification.
In the MKL framework, each layerâ€™s output representation from the WavLM Transformer encoders is treated as a separate feature space. These features are combined using a set of kernels, where each kernel is designed to capture specific characteristics of the data. The MKL approach enables the model to learn an optimal combination of these kernels, effectively fusing information from different layers to enhance classification performance.
